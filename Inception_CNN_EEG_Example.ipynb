{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "8nZPczcStB3L",
    "outputId": "eeaffa5b-6021-4d8f-e8df-c079ff222018",
    "ExecuteTime": {
     "end_time": "2024-10-17T04:38:29.991374Z",
     "start_time": "2024-10-17T04:38:29.923515Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Conv2D, MaxPooling2D, BatchNormalization, Reshape, GlobalAveragePooling2D, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def InceptionNucleus(input_layer, filters):\n",
    "    branches = []\n",
    "    for f in filters:\n",
    "        x = Conv1D(filters=32, kernel_size=f, padding='same', activation='relu')(input_layer)\n",
    "        branches.append(x)\n",
    "    output = tf.keras.layers.concatenate(branches, axis=-1)\n",
    "    return output\n",
    "\n",
    "def create_eeg_cnn(input_shape=(178, 1), num_classes=2):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Initial 1D Convolutional Layers with Inception Nucleus\n",
    "    x = InceptionNucleus(inputs, filters=[4, 8, 12])\n",
    "\n",
    "    # Reshape for 2D Convolutions\n",
    "    x = Reshape((x.shape[1], x.shape[2], 1))(x) # (178, 96, 1) for example\n",
    "\n",
    "    # 2D Convolutional Layers\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Flatten after 2D Convolutions\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_eeg_cnn(input_shape=(178, 1), num_classes=2)\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 178, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 178, 32)              160       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 178, 32)              288       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 178, 32)              416       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 178, 96)              0         ['conv1d[0][0]',              \n",
      "                                                                     'conv1d_1[0][0]',            \n",
      "                                                                     'conv1d_2[0][0]']            \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 178, 96, 1)           0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 178, 96, 64)          640       ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 178, 96, 64)          256       ['conv2d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 89, 48, 64)           0         ['batch_normalization[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 273408)               0         ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    546818    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 548578 (2.09 MB)\n",
      "Trainable params: 548450 (2.09 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# Read the CSV file\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "\n",
    "# Split the first column into three new columns\n",
    "df[['second_no', 'version', 'pid']] = df.iloc[:, 0].str.split('.', expand=True)\n",
    "\n",
    "# Extract the numeric parts and replace X and V directly\n",
    "df['second_no'] = df['second_no'].str.extract('(\\d+)').astype(int)  # Replace X with its numeric part\n",
    "df['version'] = df['version'].str.extract('(\\d+)').astype(int)  # Replace V with its numeric part\n",
    "# Convert the 'num' column to integer for sorting\n",
    "# Fill None values in 'num' with a default value (e.g., 0)\n",
    "df['pid'] = df['pid'].fillna(0).astype(int)\n",
    "# Rename the 'Unnamed: 0' column to 'id'\n",
    "df.rename(columns={'Unnamed: 0': 'id'}, inplace=True)\n",
    "\n",
    "# Reorder the columns to move num, X, V between id and X1\n",
    "df = df[['id', 'pid', 'second_no', 'version', 'X1'] + [col for col in df.columns if col not in ['id', 'pid', 'second_no', 'version', 'X1']]]\n",
    "\n",
    "# Sort the DataFrame by the 'num' column\n",
    "df_sorted = df.sort_values(by=['version', 'pid', 'second_no'])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head(50)\n",
    "\n",
    "# Find the maximum existing pid\n",
    "max_pid = df['pid'].max()\n",
    "# Identify unique versions that are not 1\n",
    "versions_with_empty_pid = df[df['version'] != 1]['version'].unique()\n",
    "# Assign a unique pid to each participant and unify the version to 1\n",
    "for i, version in enumerate(versions_with_empty_pid):\n",
    "    new_pid = max_pid + i + 1  # Start from max_pid + 1\n",
    "    df.loc[(df['version'] == version), 'pid'] = new_pid\n",
    "    df.loc[(df['version'] == version), 'version'] = 1\n",
    "\n",
    "# Binary classification: 1: Seizure, 0: Non-seizure\n",
    "# Change y value to 0 if y != 1\n",
    "df['y'] = df['y'].apply(lambda x: 0 if x != 1 else x)\n",
    "\n",
    "X = df.drop(columns=['pid', 'second_no', 'version', 'id', 'y'])  # Features\n",
    "y = df['y']  # Target variable"
   ],
   "metadata": {
    "id": "qSKbBbXdtD_l",
    "ExecuteTime": {
     "end_time": "2024-10-17T04:36:20.919482Z",
     "start_time": "2024-10-17T04:36:20.772603Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T04:42:31.339970Z",
     "start_time": "2024-10-17T04:42:31.330937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score\n",
    "\n",
    "\n",
    "def evaluate_classification_metrics(y_true, y_pred, y_pred_prob):\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    if len(set(y_true)) > 1:\n",
    "        # Check if ROC-AUC can be calculated (i.e., both classes are present)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "        # Calculate Cohen's Kappa\n",
    "        kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    else:\n",
    "        roc_auc = None  # Not computable, only one class in y_true\n",
    "        kappa = None\n",
    "    \n",
    "    # Calculate metrics for seizure class (y_label=1)\n",
    "    precision_seizure = precision_score(y_true, y_pred, pos_label=1)\n",
    "    recall_seizure = recall_score(y_true, y_pred, pos_label=1)\n",
    "    f1_seizure = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    # Calculate metrics for non-seizure class (y_label=0)\n",
    "    precision_non_seizure = precision_score(y_true, y_pred, pos_label=0)\n",
    "    recall_non_seizure = recall_score(y_true, y_pred, pos_label=0)\n",
    "    f1_non_seizure = f1_score(y_true, y_pred, pos_label=0)\n",
    "    \n",
    "    print(f'\\nSeizure (y=1):')\n",
    "    print(f'  Precision: {precision_seizure * 100:.2f} %')\n",
    "    print(f'  Recall: {recall_seizure * 100:.2f} %')\n",
    "    print(f'  F1 Score: {f1_seizure * 100:.2f} %')\n",
    "    \n",
    "    print(f'\\nNon-Seizure (y=0):')\n",
    "    print(f'  Precision: {precision_non_seizure * 100:.2f} %')\n",
    "    print(f'  Recall: {recall_non_seizure * 100:.2f} %')\n",
    "    print(f'  F1 Score: {f1_non_seizure * 100:.2f} %')\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'\\nOverall:')\n",
    "    print(f'  Accuracy: {accuracy * 100:.2f} %')\n",
    "    print(f'  Precision: {precision * 100:.2f} %')\n",
    "    print(f'  Recall: {recall * 100:.2f} %')\n",
    "    print(f'  F1 Score: {f1 * 100:.2f} %')\n",
    "    if roc_auc is not None:\n",
    "        print(f'  ROC-AUC: {roc_auc * 100:.2f} %')\n",
    "    if kappa is not None:\n",
    "        print(f'  Cohen\\'s Kappa: {kappa * 100:.2f} %')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T04:59:34.732788Z",
     "start_time": "2024-10-17T04:48:33.024412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "X_reshaped = np.array(X).reshape(-1, 178, 1)\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the model\n",
    "model = create_eeg_cnn(input_shape=(178, 1), num_classes=2)\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),  # For evaluation during training\n",
    "    epochs=30,                         # Number of epochs\n",
    "    batch_size=32,                     # Batch size (adjust based on memory)\n",
    "    callbacks=[early_stopping]          # Early stopping\n",
    ")\n",
    "\n",
    "# After training, evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Predict class probabilities for the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary class labels (assuming threshold 0.5)\n",
    "y_pred = (y_pred_prob[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Evaluate classification metrics using the provided function\n",
    "evaluate_classification_metrics(y_test, y_pred, y_pred_prob[:, 1])\n",
    "\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "288/288 [==============================] - 49s 168ms/step - loss: 9.5476 - accuracy: 0.9237 - val_loss: 3.8512 - val_accuracy: 0.9587\n",
      "Epoch 2/30\n",
      "288/288 [==============================] - 48s 167ms/step - loss: 3.2597 - accuracy: 0.9530 - val_loss: 3.3574 - val_accuracy: 0.9574\n",
      "Epoch 3/30\n",
      "288/288 [==============================] - 47s 164ms/step - loss: 2.4595 - accuracy: 0.9604 - val_loss: 4.7436 - val_accuracy: 0.9535\n",
      "Epoch 4/30\n",
      "288/288 [==============================] - 48s 166ms/step - loss: 1.7028 - accuracy: 0.9699 - val_loss: 2.1359 - val_accuracy: 0.9613\n",
      "Epoch 5/30\n",
      "288/288 [==============================] - 48s 167ms/step - loss: 1.2081 - accuracy: 0.9733 - val_loss: 2.2791 - val_accuracy: 0.9343\n",
      "Epoch 6/30\n",
      "288/288 [==============================] - 49s 171ms/step - loss: 0.5948 - accuracy: 0.9836 - val_loss: 3.2205 - val_accuracy: 0.9243\n",
      "Epoch 7/30\n",
      "288/288 [==============================] - 50s 174ms/step - loss: 0.7680 - accuracy: 0.9825 - val_loss: 2.8094 - val_accuracy: 0.9591\n",
      "Epoch 8/30\n",
      "288/288 [==============================] - 51s 177ms/step - loss: 0.4964 - accuracy: 0.9866 - val_loss: 0.8887 - val_accuracy: 0.9796\n",
      "Epoch 9/30\n",
      "288/288 [==============================] - 52s 181ms/step - loss: 0.5948 - accuracy: 0.9834 - val_loss: 1.2623 - val_accuracy: 0.9604\n",
      "Epoch 10/30\n",
      "288/288 [==============================] - 54s 187ms/step - loss: 0.3787 - accuracy: 0.9879 - val_loss: 1.2759 - val_accuracy: 0.9748\n",
      "Epoch 11/30\n",
      "288/288 [==============================] - 53s 184ms/step - loss: 0.2628 - accuracy: 0.9901 - val_loss: 1.1587 - val_accuracy: 0.9730\n",
      "Epoch 12/30\n",
      "288/288 [==============================] - 53s 184ms/step - loss: 0.2148 - accuracy: 0.9916 - val_loss: 1.0807 - val_accuracy: 0.9783\n",
      "Epoch 13/30\n",
      "288/288 [==============================] - 53s 185ms/step - loss: 0.1565 - accuracy: 0.9926 - val_loss: 1.0071 - val_accuracy: 0.9687\n",
      "72/72 [==============================] - 3s 42ms/step - loss: 0.8887 - accuracy: 0.9796\n",
      "72/72 [==============================] - 3s 42ms/step\n",
      "\n",
      "Seizure (y=1):\n",
      "  Precision: 93.72 %\n",
      "  Recall: 96.34 %\n",
      "  F1 Score: 95.02 %\n",
      "\n",
      "Non-Seizure (y=0):\n",
      "  Precision: 99.07 %\n",
      "  Recall: 98.37 %\n",
      "  F1 Score: 98.71 %\n",
      "\n",
      "Overall:\n",
      "  Accuracy: 97.96 %\n",
      "  Precision: 93.72 %\n",
      "  Recall: 96.34 %\n",
      "  F1 Score: 97.97 %\n",
      "  ROC-AUC: 98.85 %\n",
      "  Cohen's Kappa: 93.73 %\n",
      "Test Loss: 0.8887413740158081\n",
      "Test Accuracy: 0.9795652031898499\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
